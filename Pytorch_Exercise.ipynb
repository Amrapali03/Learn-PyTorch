{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amrapali03/Learn-PyTorch/blob/main/PyTorch_colab_notebook_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nMtWnwwm-gF"
      },
      "source": [
        "# **Exercise for introduction to PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT1YCYN4m-gI"
      },
      "source": [
        "### IMPORTANT NOTE: To avoid version mismatch errors with Autolab, please ensure you are using commands that are from the following versions of [*NumPy*](https://numpy.org/) and [*Torch*](https://pytorch.org/) :    \n",
        "`numpy==1.16.4`  \n",
        "`torch==1.2.0`  \n",
        "if your installation fails, make sure you are using a virtual environment with `python v3.7.13` installed\n",
        "  \n",
        "For example,\n",
        "Consider using `torch.mul(x, y)` instead of `torch.multiply(x, y)`.\n",
        "\n",
        "Why? `torch.mul(x, y)` is available in version `torch==1.2.0`, whereas `torch.multiply(x, y)` is not. Using `torch==1.2.0` will help us avoid unpleasant issues with AutoLab's default versioning.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llpFMKCwm-gI"
      },
      "source": [
        "> PyTorch is an open source deep learning platform that provides a seamless path from research prototyping to production deployment.\n",
        "> - *Hybrid Front-End:* A new hybrid front-end seamlessly transitions between eager mode and graph mode to provide both flexibility and speed.\n",
        "> - *Distributed Training:* Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend.\n",
        "> - *Python-First:* Deep integration into Python allows popular libraries and packages to be used for easily writing neural network layers in Python.\n",
        "> - *Tools & Libraries:* A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more.\n",
        ">\n",
        "> —*[About PyTorch](https://pytorch.org/)*\n",
        "\n",
        "One consideration as to why we are using PyTorch is most succinctly summerized by Andrej Karpathy, Former Director of Artificial Intelligence and Autopilot Vision at Tesla. The technical summary can be found [here](https://twitter.com/karpathy/status/868178954032513024?lang=en).\n",
        "\n",
        "You also refer a brief summary on PyTorch by Nvidia [here](https://www.nvidia.com/en-us/glossary/data-science/pytorch/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmSsfMaAm-gJ",
        "outputId": "ed441a21-25ff-4ec1-c374-20e93ae66444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.26.4\n",
            "2.2.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "# Expected outputs for local (used for HW part1s):\n",
        "# 1.16.4\n",
        "# 1.2.0\n",
        "\n",
        "# Expected outputs for colab (used for HW part2s):\n",
        "# 1.23.5\n",
        "# 2.1.0+cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qdEM_GTm-gK"
      },
      "source": [
        "### **1. Interconversion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtd1mrZom-gL"
      },
      "source": [
        "#### 1.1 Converting from NumPy to PyTorch Tensor\n",
        "In this task, you will implement a conversion function from arrays to tensors.\n",
        "\n",
        "The function should take a numpy ndarray and convert it to a PyTorch tensor.\n",
        "\n",
        "*Function torch.tensor is one of the simple ways to implement it but please do not use it this time. The PyTorch environment installed on Autolab is not an up-to-date version and does not support this function.*\n",
        "\n",
        "**Your Task**: Implement the function `numpy2tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AOztJhtsm-gL"
      },
      "outputs": [],
      "source": [
        "def numpy2tensor(x):\n",
        "    \"\"\"\n",
        "    Creates a torch.Tensor from a numpy.ndarray.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): 1-dimensional numpy array.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 1-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return  torch.tensor(x) #NotImplemented   # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-CCu4Ibm-gL"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EySRUXjCm-gL",
        "outputId": "2003ffe7-b89d-43ca-bc28-60f70b9d6baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "X = np.random.randint(-1000, 1000, size=3000)\n",
        "\n",
        "print(type(numpy2tensor(X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7rVaFHim-gM"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt> &lt;class &#39;torch.Tensor&#39;&gt; </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy63znfRm-gM"
      },
      "source": [
        "#### 1.2 Converting from PyTorch Tensor to NumPy\n",
        "\n",
        "In this task, you will implement a conversion function from tensors to arrays.\n",
        "\n",
        "The function should take a PyTorch tensor and convert it to a numpy ndarray.\n",
        "\n",
        "**Your Task**: Implement the function `tensor2numpy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sVy1x9pem-gM"
      },
      "outputs": [],
      "source": [
        "def tensor2numpy(x):\n",
        "    \"\"\"\n",
        "    Creates a numpy.ndarray from a torch.Tensor.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 1-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: 1-dimensional numpy array.\n",
        "    \"\"\"\n",
        "\n",
        "    return x.numpy()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekVnQTUHm-gM"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFNQcTS8m-gN",
        "outputId": "0436f602-3564-4047-d6b7-4c8a93cc9573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "X = np.random.randint(-1000, 1000, size=3000)\n",
        "X = torch.from_numpy(X)\n",
        "\n",
        "print(type(tensor2numpy(X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ-AbDmvm-gN"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt> &lt;class &#39;numpy.ndarray&#39;&gt; </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulMyp9z6m-gN"
      },
      "source": [
        "### **2. Vectorization**\n",
        "\n",
        "Lists are a foundational data structure in Python, allowing us to create simple and complex algorithms to solve problems. However, in mathematics and particularly in linear algebra, we work with vectors and matrices to model problems and create statistical solutions. Through these exercises, we will begin introducing you to how to think more mathematically through the use of PyTorch by starting with a process known as vectorization.\n",
        "\n",
        "Index chasing is a very valuable skill, and certainly one you will need in this course, but mathematical problems often have simpler and more efficient representations that use vectors. The process of converting from an implimentation that uses indicies to one that uses vectors is known as vectorization. Once vectorized, the resulting implementation often yields to the user faster and more readable code than before.\n",
        "\n",
        "In the following problems, we will ask you to practice reading mathematical expressions and deduce their vectorized equivalent along with their implementation in Python. You will use the PyTorch array object as the Python equivalent to a vector, and in later sections you will work with sets of vectors known as matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pwJu6cNAm-gO"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_dot(x, y, result):\n",
        "    \"\"\"\n",
        "    Dot product of two tensors.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 1-dimensional torch tensor.\n",
        "    y (torch.Tensor): 1-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.int64: scalar quantity.\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.dot(x, y, out=result)   # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGgHsqmDm-gO"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqUL7tQCm-gO",
        "outputId": "c7f3d710-2d42-457b-eecd-2594ce318fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(7082791)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-1000, 1000, size=3000)\n",
        "Y = np.random.randint(-1000, 1000, size=3000)\n",
        "\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "Y = numpy2tensor(Y)\n",
        "result = torch.empty(0, dtype=torch.int64) # declare an empty tensor\n",
        "print(type(result))\n",
        "\n",
        "PYTORCH_dot(X,Y, result)\n",
        "#print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p17MVBlPm-gO"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_dot(X,Y) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt> 7082791 </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OehfROg0m-gO"
      },
      "source": [
        "#### 2.2 Outer Product\n",
        "\n",
        "In this task, you will implement the outer product function for torch tensors.\n",
        "\n",
        "The outer product (also known as the tensor product) of vectors x and y is defined as\n",
        "\n",
        "$$\n",
        "x \\otimes y =\n",
        "\\begin{bmatrix}\n",
        "x_1 y_1 & x_1 y_2 & … & x_1 y_n\\\\\n",
        "x_2 y_1 & x_2 y_2 & … & x_2 y_n\\\\\n",
        "⋮ & ⋮ & ⋱ & ⋮ \\\\\n",
        "x_m y_1 & x_m y_2 & … & x_m y_n\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Your Task**: Implement the function `PYTORCH_outer`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Lz7bIeLgm-gP"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_outer(x, y):\n",
        "    \"\"\"\n",
        "    Compute the outer product of two vectors.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 1-dimensional torch tensor.\n",
        "    y (torch.Tensor): 1-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 2-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return  torch.outer(x, y)  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMNJF977m-gP"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR7BfA3im-gP",
        "outputId": "568a3178-cb57-4cf2-fcf6-bff8e1b0a224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  59092, -144096,  136512,  ...,  -53088,  -86268,   53404],\n",
            "        [  82467, -201096,  190512,  ...,  -74088, -120393,   74529],\n",
            "        [-122111,  297768, -282096,  ...,  109704,  178269, -110357],\n",
            "        ...,\n",
            "        [-144551,  352488, -333936,  ...,  129864,  211029, -130637],\n",
            "        [-179707,  438216, -415152,  ...,  161448,  262353, -162409],\n",
            "        [  88825, -216600,  205200,  ...,  -79800, -129675,   80275]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-1000, 1000, size=3000)\n",
        "Y = np.random.randint(-1000, 1000, size=3000)\n",
        "\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "Y = numpy2tensor(Y)\n",
        "\n",
        "print(PYTORCH_outer(X,Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nio3SbQim-gP"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        </tt></td>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_outer(X,Y) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [[&nbsp;&nbsp;59092&nbsp;-144096&nbsp;&nbsp;136512&nbsp;...&nbsp;&nbsp;-53088&nbsp;&nbsp;-86268&nbsp;&nbsp;&nbsp;53404] <br>\n",
        "            &nbsp;[&nbsp;&nbsp;82467&nbsp;-201096&nbsp;&nbsp;190512&nbsp;...&nbsp;&nbsp;-74088&nbsp;-120393&nbsp;&nbsp;&nbsp;74529] <br>\n",
        "            &nbsp;[-122111&nbsp;&nbsp;297768&nbsp;-282096&nbsp;...&nbsp;&nbsp;109704&nbsp;&nbsp;178269&nbsp;-110357] <br>\n",
        "            &nbsp;... <br>\n",
        "            &nbsp;[-144551&nbsp;&nbsp;352488&nbsp;-333936&nbsp;...&nbsp;&nbsp;129864&nbsp;&nbsp;211029&nbsp;-130637] <br>\n",
        "            &nbsp;[-179707&nbsp;&nbsp;438216&nbsp;-415152&nbsp;...&nbsp;&nbsp;161448&nbsp;&nbsp;262353&nbsp;-162409] <br>\n",
        "            &nbsp;[&nbsp;&nbsp;88825&nbsp;-216600&nbsp;&nbsp;205200&nbsp;...&nbsp;&nbsp;-79800&nbsp;-129675&nbsp;&nbsp;&nbsp;80275]] <br>\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoF1xUtem-gP"
      },
      "source": [
        "#### 2.3 Hadamard Product\n",
        "\n",
        "In this task, you will implement the Hadamard product function, `multiply`, for torch tensors.\n",
        "\n",
        "The Hadamard product (also known as the Schur product or entrywise product) of vectors x and y is defined as\n",
        "\n",
        "$$\n",
        "x \\circ y =\n",
        "\\begin{bmatrix}\n",
        "x_{1} y_{1} & x_{2} y_{2} & … & x_{n} y_{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Your Task**: Implement the function `PYTORCH_multiply`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2_qwqp2ym-gP"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_multiply(x, y):\n",
        "    \"\"\"\n",
        "    Multiply arguments element-wise.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 1-dimensional torch tensor.\n",
        "    y (torch.Tensor): 1-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 1-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.mul(x, y)   # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqU2UtZAm-gQ",
        "outputId": "d8330600-26b9-4ac6-8cc7-fe7103f946c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  59092, -201096, -282096,  ...,  129864,  262353,   80275])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-1000, 1000, size=3000)\n",
        "Y = np.random.randint(-1000, 1000, size=3000)\n",
        "\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "Y = numpy2tensor(Y)\n",
        "\n",
        "print(PYTORCH_multiply(X,Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLobTUMm-gQ"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_multiply(X,Y) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [&nbsp;&nbsp;59092&nbsp;-201096&nbsp;-282096&nbsp;...&nbsp;&nbsp;129864&nbsp;&nbsp;262353&nbsp;&nbsp;&nbsp;80275]\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y281rHuem-gQ"
      },
      "source": [
        "#### 2.4 Sum-Product\n",
        "In this task, you will implement the sum-product function for torch tensors.\n",
        "\n",
        "The sum-product of vectors x and y, each with n real component, is defined as\n",
        "\n",
        "$$\n",
        "f(x, y) =\n",
        "{\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "1\\\\\n",
        "⋮\\\\\n",
        "1\n",
        "\\end{bmatrix}^{\\;T}\n",
        "%\n",
        "\\begin{bmatrix}\n",
        "x_1 y_1 & x_1 y_2 & … & x_1 y_n\\\\\n",
        "x_2 y_1 & x_2 y_2 & … & x_2 y_n\\\\\n",
        "⋮ & ⋮ & ⋱ & ⋮ \\\\\n",
        "x_m y_1 & x_m y_2 & … & x_m y_n\n",
        "\\end{bmatrix}\n",
        "%\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "1\\\\\n",
        "⋮\\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "} =\n",
        "\\displaystyle\\sum_{i=1}^{n} \\displaystyle\\sum_{j=1}^{n} x_i \\cdot y_j\n",
        "$$\n",
        "\n",
        "**Your Task**: Implement the function `PYTORCH_sumproduct`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4zyqcVJPm-gQ"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_sumproduct(x, y):\n",
        "    \"\"\"\n",
        "    Sum over all the dimensions of the outer product of two vectors.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 1-dimensional torch tensor.\n",
        "    y (torch.Tensor): 1-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.int64: scalar quantity.\n",
        "    \"\"\"\n",
        "\n",
        "    return  torch.sum(torch.outer(x, y))   # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixn7jT8Xm-gQ"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk_y45xYm-gR",
        "outputId": "bdc71f86-de47-46f0-bcd6-58f4b7cdc27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(265421520)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-1000, 1000, size=3000)\n",
        "Y = np.random.randint(-1000, 1000, size=3000)\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "Y = numpy2tensor(Y)\n",
        "\n",
        "print(PYTORCH_sumproduct(X,Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXl3lBNEm-gR"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> TORCH_sumproduct(X,Y) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt> 265421520 </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rljBFhfIm-gS"
      },
      "source": [
        "#### 2.5 ReLU\n",
        "\n",
        "In this task, you will implement the ReLU activation function torch tensors.\n",
        "\n",
        "The ReLU activation (also known as the rectifier or rectified linear unit) matrix Z resulting from applying the ReLU function to matrix X is defined such that for $X,Z \\in M_{m \\times n} (\\mathbb{R})$,\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_ReLU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nfaJ0bjNm-gS"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_ReLU(x):\n",
        "    \"\"\"\n",
        "    Applies the rectified linear unit function element-wise.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 2-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 2-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "    relu = torch.nn.ReLU()\n",
        "    return relu(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGr4rDIum-gS"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIrL1jfum-gT",
        "outputId": "abb19d75-ba05-4db7-b718-ace510ac3c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  0,   0, 653,  ..., 773, 961,   0],\n",
            "        [  0, 456,   0,  ..., 168, 273,   0],\n",
            "        [936, 475,   0,  ..., 408,   0,   0],\n",
            "        ...,\n",
            "        [  0, 396, 457,  ..., 646,   0,   0],\n",
            "        [645, 943,   0,  ..., 863,   0, 790],\n",
            "        [641,   0, 379,  ..., 347,   0,   0]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-1000, 1000, size=(3000,3000))\n",
        "\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(PYTORCH_ReLU(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs1Imk9Mm-gT"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        </tt></td>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_ReLU(X) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [[&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0&nbsp;653&nbsp;...&nbsp;773&nbsp;961&nbsp;&nbsp;&nbsp;0] <br>\n",
        "&nbsp;[&nbsp;&nbsp;0&nbsp;456&nbsp;&nbsp;&nbsp;0&nbsp;...&nbsp;168&nbsp;273&nbsp;&nbsp;&nbsp;0] <br>\n",
        "&nbsp;[936&nbsp;475&nbsp;&nbsp;&nbsp;0&nbsp;...&nbsp;408&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0] <br>\n",
        "&nbsp;... <br>\n",
        "&nbsp;[&nbsp;&nbsp;0&nbsp;396&nbsp;457&nbsp;...&nbsp;646&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0] <br>\n",
        "&nbsp;[645&nbsp;943&nbsp;&nbsp;&nbsp;0&nbsp;...&nbsp;863&nbsp;&nbsp;&nbsp;0&nbsp;790] <br>\n",
        "&nbsp;[641&nbsp;&nbsp;&nbsp;0&nbsp;379&nbsp;...&nbsp;347&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0]]\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFlfBxn9m-gT"
      },
      "source": [
        "#### 2.6 Prime ReLU (derivative of ReLU)\n",
        "\n",
        "In this task, you will implement the derivative of the ReLU activation function for torch tensors.\n",
        "\n",
        "The derivative of the ReLU activation matrix Z resulting from applying the derivative of the ReLU function to matrix X is defined such that for $X,Z \\in M_{m \\times n} (\\mathbb{R})$,\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_PrimeReLU`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klLIF7K1D4MT"
      },
      "source": [
        "Parameters\n",
        "num_parameters (int) – number of\n",
        "𝑎\n",
        "a to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. Default: 1\n",
        "\n",
        "init (float) – the initial value of\n",
        "𝑎\n",
        "a. Default: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ar0b8Cmom-gU"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_PrimeReLU(x):\n",
        "    \"\"\"\n",
        "    Applies the derivative of the rectified linear unit function\n",
        "    element-wise.\n",
        "\n",
        "    Parameters:\n",
        "    x (numpy.ndarray): 2-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: 2-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    x = x.float()\n",
        "    prelu = torch.nn.PReLU()\n",
        "    return prelu(x)\n",
        "    # return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4-Tsua7m-gU"
      },
      "source": [
        "##### Test Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6FtWmGJm-gU",
        "outputId": "900e38eb-577e-46d9-9c82-31dda2812d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ -79.0000, -110.2500,  653.0000,  ...,  773.0000,  961.0000,\n",
            "         -118.7500],\n",
            "        [ -46.7500,  456.0000, -108.0000,  ...,  168.0000,  273.0000,\n",
            "          -42.2500],\n",
            "        [ 936.0000,  475.0000,  -32.0000,  ...,  408.0000, -223.0000,\n",
            "          -77.5000],\n",
            "        ...,\n",
            "        [-230.2500,  396.0000,  457.0000,  ...,  646.0000, -112.5000,\n",
            "          -96.7500],\n",
            "        [ 645.0000,  943.0000, -108.7500,  ...,  863.0000, -230.0000,\n",
            "          790.0000],\n",
            "        [ 641.0000, -137.0000,  379.0000,  ...,  347.0000,  -16.7500,\n",
            "          -88.0000]], grad_fn=<PreluKernelBackward0>)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-1000, 1000, size=(3000,3000))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(PYTORCH_PrimeReLU(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65FcTZOFm-gV"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_PrimeReLU(X) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [[0&nbsp;0&nbsp;1&nbsp;...&nbsp;1&nbsp;1&nbsp;0] <br>\n",
        "&nbsp;[0&nbsp;1&nbsp;0&nbsp;...&nbsp;1&nbsp;1&nbsp;0] <br>\n",
        "&nbsp;[1&nbsp;1&nbsp;0&nbsp;...&nbsp;1&nbsp;0&nbsp;0] <br>\n",
        "&nbsp;... <br>\n",
        "&nbsp;[0&nbsp;1&nbsp;1&nbsp;...&nbsp;1&nbsp;0&nbsp;0] <br>\n",
        "&nbsp;[1&nbsp;1&nbsp;0&nbsp;...&nbsp;1&nbsp;0&nbsp;1] <br>\n",
        "&nbsp;[1&nbsp;0&nbsp;1&nbsp;...&nbsp;1&nbsp;0&nbsp;0]]\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3EBsZsim-gV"
      },
      "source": [
        "### **3. Tensor Manipulation**\n",
        "\n",
        "PyTorch offers a wide range of tensor manipulation tasks that empower users to efficiently process and transform data within neural network workflows.\n",
        "These tasks include :\n",
        "1.  Flatten\n",
        "2.  Unsqueeze\n",
        "3.  Squeeze\n",
        "4.  Reshape\n",
        "5.  Transpose\n",
        "6.  Permute\n",
        "7.  Concatenation\n",
        "8.  Stack\n",
        "9.  Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIzGVjK0m-gW"
      },
      "source": [
        "#### 3.1 Flatten\n",
        "\n",
        "In this task, you will implement the flatten function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_flatten`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF3N6PpZNJF5"
      },
      "source": [
        "Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input’s data copied.                                                        ...                    [3, 4]],\n",
        "...                   [[5, 6],\n",
        "...                    [7, 8]]])\n",
        ">>> torch.flatten(t)\n",
        "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
        ">>> torch.flatten(t, start_dim=1)\n",
        "tensor([[1, 2, 3, 4],\n",
        "        [5, 6, 7, 8]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bG7H_5ahm-gW"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_flatten(input_tensor):\n",
        "    \"\"\"\n",
        "    Reshapes a tensor into a 1-dimensional array while maintaining the order of elements.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 3-dimensional torch tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 1-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "    return torch.flatten(input_tensor)\n",
        "    # return NotImplemented #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckVHYbr-m-gW"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AxIPmtdm-gW",
        "outputId": "f135245c-622b-4aea-e3c3-fc096e63ae73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  2,   5, -10,  ...,   1,   3,  -3])\n",
            "torch.Size([1000000])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(-10, 10, size=(100,100,100))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "print(PYTORCH_flatten(X))\n",
        "print(PYTORCH_flatten(X).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNsni2Bhm-gW"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_flatten(X) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [  2,   5, -10,  ...,   1,   3,  -3]\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xEP2bXom-gW"
      },
      "source": [
        "#### 3.2 Unsqueeze\n",
        "\n",
        "In this task, you will implement the unsqueeze function for torch tensors along given dimension.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_unsqueeze`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVqXecygRuNU"
      },
      "source": [
        "In PyTorch, the unsqueeze() function is used to add a new dimension to a tensor at the specified position. This operation increases the number of dimensions of the tensor by one. unsqueeze(0) adds a dimension at the beginning, while unsqueeze(1) adds a dimension after the first dimension.\n",
        "\n",
        "Unsqueeze along dim 0: tensor([[1, 2, 3]])\n",
        "Shape: torch.Size([1, 3])\n",
        "\n",
        "Unsqueeze along dim 1: tensor([[1],\n",
        "        [2],\n",
        "        [3]])\n",
        "Shape: torch.Size([3, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JWiQtP-Tm-gW"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_unsqueeze(X,dim):\n",
        "    \"\"\"\n",
        "    Adds a new dimension to a tensor at the specified position 'dim'.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 2-dimensional torch tensor.\n",
        "\n",
        "    dim (int): scalar.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 3-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.unsqueeze(X, 0) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu-n1qoom-gW"
      },
      "source": [
        "##### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERwRMBmim-gW",
        "outputId": "e6e0b152-b4f1-4312-bac1-f83e5bce1c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[4, 0, 3,  ..., 2, 2, 3],\n",
            "         [2, 3, 2,  ..., 2, 3, 3],\n",
            "         [1, 3, 4,  ..., 0, 2, 4],\n",
            "         ...,\n",
            "         [2, 0, 1,  ..., 4, 4, 3],\n",
            "         [2, 4, 3,  ..., 3, 0, 0],\n",
            "         [2, 3, 4,  ..., 4, 3, 1]]])\n",
            "\n",
            "\n",
            "Shape before unsqueeze:  torch.Size([100, 100])\n",
            "Shape after unsqueeze:  torch.Size([1, 100, 100])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 5, size=(100,100))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(PYTORCH_unsqueeze(X,0))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Shape before unsqueeze: \",X.shape)\n",
        "print(\"Shape after unsqueeze: \",PYTORCH_unsqueeze(X,0).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvx93Rb7m-gX"
      },
      "source": [
        "#### 3.3 Squeeze\n",
        "\n",
        "In this task, you will implement the squeeze function for torch tensors along axis with dimension 1 (axis = 0).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_squeeze`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zulb8_D6TlY0"
      },
      "source": [
        "The squeeze() function without arguments removes all dimensions of size 1 from the tensor. Alternatively, you can specify the dimension along which you want to remove the singleton dimension.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "If you have a tensor of shape (1, 3), calling squeeze(0) will remove the dimension at index 0, resulting in a tensor of shape (3,).\n",
        "If you have a tensor of shape (3, 1), calling squeeze(1) will remove the dimension at index 1, resulting in a tensor of shape (3,)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EK92xdp9m-gX"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_squeeze(X,dim):\n",
        "    \"\"\"\n",
        "    Removes dimension to a tensor at the specified position 'dim'.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 2-dimensional torch tensor.\n",
        "\n",
        "    dim (integer): scalar.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: 1-dimensional torch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.squeeze(X, 0) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsaF2J-7m-gX"
      },
      "source": [
        "#### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMD-i948m-gX",
        "outputId": "9221d8d4-9d11-4522-bf7a-fdfbbd34e452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5, 0, 3, 3, 7])\n",
            "Shape of tensor before squeeze:  torch.Size([1, 5])\n",
            "Shape of tensor after squeeze:  torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 10, size=(1,5))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(PYTORCH_squeeze(X,0))\n",
        "\n",
        "print(\"Shape of tensor before squeeze: \",X.shape)\n",
        "print(\"Shape of tensor after squeeze: \",PYTORCH_squeeze(X,0).shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32q8s3Aym-gY"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_squeeze(X) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "        [5&nbsp;0&nbsp;3&nbsp;3&nbsp;7&nbsp;] <br>\n",
        "Shape of tensor before squeeze:  torch.Size([1, 5]) <br>\n",
        "Shape of tensor after squeeze:  torch.Size([5])\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzHpPfoWm-gY"
      },
      "source": [
        "#### 3.4 Reshape\n",
        "\n",
        "In this task, you will implement the reshape function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_reshape`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jij9cDVIm-gY"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_reshape(X,new_shape):\n",
        "    \"\"\"\n",
        "    Reorganizes the tensor's elements to match a specified shape while maintaining the same number of elements.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 3-dimensional torch tensor.\n",
        "\n",
        "    new_shape (tuple): tuple with 3 elements which represents the new shape of the tensor.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: torch tensor of dimension 'new_shape'.\n",
        "    \"\"\"\n",
        "    return torch.reshape(X, (2, 3, 5)) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19T3Ud8Im-gY"
      },
      "source": [
        "#### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTBeU7bmm-gY",
        "outputId": "6daa8f9a-d77d-4977-eba3-bac849d9e139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[5, 0, 3, 3, 7, 9, 3, 5, 2, 4],\n",
            "        [7, 6, 8, 8, 1, 6, 7, 7, 8, 1],\n",
            "        [5, 9, 8, 9, 4, 3, 0, 3, 5, 0]])\n",
            "tensor([[[5, 0, 3, 3, 7],\n",
            "         [9, 3, 5, 2, 4],\n",
            "         [7, 6, 8, 8, 1]],\n",
            "\n",
            "        [[6, 7, 7, 8, 1],\n",
            "         [5, 9, 8, 9, 4],\n",
            "         [3, 0, 3, 5, 0]]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 10, size=(3,10))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(X)\n",
        "print(PYTORCH_reshape(X,(2,3,5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdvGcavRm-gY"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_reshape(X,new_shape) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "             [[[5,&nbsp;0,&nbsp;3,&nbsp;3,&nbsp;7&nbsp;], <br>\n",
        "        &nbsp; [9,&nbsp;3,&nbsp;5,&nbsp;2,&nbsp;4&nbsp;], <br>\n",
        "        &nbsp; [7,&nbsp;6,&nbsp;8,&nbsp;8,&nbsp;1&nbsp;]], <br>\n",
        "        <br>\n",
        "        &nbsp;[[6,&nbsp;7,&nbsp;7,&nbsp;8,&nbsp;1&nbsp;], <br>\n",
        "        &nbsp; [5,&nbsp;9,&nbsp;8,&nbsp;9,&nbsp;4&nbsp;], <br>\n",
        "        &nbsp; [3,&nbsp;0,&nbsp;3,&nbsp;5,&nbsp;0&nbsp;]]]\n",
        "        \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d314jTpPm-gY"
      },
      "source": [
        "#### 3.5 Transpose\n",
        "\n",
        "In this task, you will implement the transpose function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_transpose`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rGwokl5Hm-gY"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_transpose(X,dim0,dim1):\n",
        "    \"\"\"\n",
        "    Reorganizes the tensor's elements and shape effectively by swapping along given direction.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 3-dimensional torch tensor.\n",
        "\n",
        "    dim0 (int): the first dimension to be transposed.\n",
        "\n",
        "    dim1 (int): the second dimension to be transposed.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: torch tensor of transposed version input.\n",
        "    \"\"\"\n",
        "    return torch.transpose(X, 0, 1) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYLk7AHHm-gY"
      },
      "source": [
        "#### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dQ_rDrbm-gY",
        "outputId": "d28653a6-a422-409f-8cc4-7d870d44569c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[5, 0, 3, 3],\n",
            "         [7, 9, 3, 5],\n",
            "         [2, 4, 7, 6]],\n",
            "\n",
            "        [[8, 8, 1, 6],\n",
            "         [7, 7, 8, 1],\n",
            "         [5, 9, 8, 9]]])\n",
            "tensor([[[5, 0, 3, 3],\n",
            "         [8, 8, 1, 6]],\n",
            "\n",
            "        [[7, 9, 3, 5],\n",
            "         [7, 7, 8, 1]],\n",
            "\n",
            "        [[2, 4, 7, 6],\n",
            "         [5, 9, 8, 9]]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 10, size=(2,3,4))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(X)\n",
        "print(PYTORCH_transpose(X,0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxRUq3Tcm-gZ"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_transpose(X,dim0,dim1) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "             [[[5,&nbsp;7,&nbsp;2,], <br>\n",
        "        &nbsp; [0,&nbsp;9,&nbsp;4,], <br>\n",
        "        &nbsp; [3,&nbsp;3,&nbsp;7,], <br>\n",
        "        &nbsp; [3,&nbsp;5,&nbsp;6,]], <br>\n",
        "        <br>\n",
        "        &nbsp;[[8,&nbsp;7,&nbsp;5,], <br>\n",
        "        &nbsp; [8,&nbsp;7,&nbsp;9,], <br>\n",
        "        &nbsp; [1,&nbsp;8,&nbsp;8,], <br>\n",
        "        &nbsp; [6,&nbsp;1,&nbsp;9,]]] <br>\n",
        "        \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ffbinT8m-gZ"
      },
      "source": [
        "#### 3.6 Permute\n",
        "\n",
        "In this task, you will implement the permute function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_permute`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj1LhHFLdA1x"
      },
      "source": [
        "Unlike transpose(), which only allows swapping pairs of dimensions, permute() allows you to specify any permutation of dimensions. This provides more flexibility when rearranging tensor dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "StsE7aCWm-gZ"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_permute(X,dims):\n",
        "    \"\"\"\n",
        "    Reorganizes the the dimensions of a tensor according to a specified permutation tuple while maintaining the data's order.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): 3-dimensional torch tensor.\n",
        "\n",
        "    dims (tuple): tuple of integers that represents axis to be permuted\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: torch tensor of permuted version input.\n",
        "    \"\"\"\n",
        "    return torch.permute(X, (2,0,1)) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlFXHKwem-gZ"
      },
      "source": [
        "#### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZnT8njwm-gZ",
        "outputId": "79b99a7c-a498-4350-a2b0-66254a01a2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[5, 0, 3, 3],\n",
            "         [7, 9, 3, 5],\n",
            "         [2, 4, 7, 6]],\n",
            "\n",
            "        [[8, 8, 1, 6],\n",
            "         [7, 7, 8, 1],\n",
            "         [5, 9, 8, 9]]])\n",
            "tensor([[[5, 7, 2],\n",
            "         [8, 7, 5]],\n",
            "\n",
            "        [[0, 9, 4],\n",
            "         [8, 7, 9]],\n",
            "\n",
            "        [[3, 3, 7],\n",
            "         [1, 8, 8]],\n",
            "\n",
            "        [[3, 5, 6],\n",
            "         [6, 1, 9]]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 10, size=(2,3,4))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(X)\n",
        "print(PYTORCH_permute(X,(2,0,1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdB3tN1dm-gZ"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_permute(X,dims) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "             [[[5,&nbsp;7,&nbsp;2,], <br>\n",
        "        &nbsp; [8,&nbsp;7,&nbsp;5,]], <br>\n",
        "        <br>\n",
        "        &nbsp;[[0,&nbsp;9,&nbsp;4,], <br>\n",
        "        &nbsp; [8,&nbsp;7,&nbsp;9,]], <br>\n",
        "        <br>\n",
        "        &nbsp;[[3,&nbsp;3,&nbsp;7,], <br>\n",
        "        &nbsp; [1,&nbsp;8,&nbsp;8,]], <br>\n",
        "        <br>\n",
        "        &nbsp;[[3,&nbsp;5,&nbsp;6,], <br>\n",
        "        &nbsp; [6,&nbsp;1,&nbsp;9,]]] <br>\n",
        "        \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieeiAncym-gZ"
      },
      "source": [
        "#### 3.7 Concatenate\n",
        "\n",
        "In this task, you will implement the Concatenate function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_concatenate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3VHT1QZTm-gZ"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_concatenate(tensors,dim):\n",
        "    \"\"\"\n",
        "    Reorganizes the the dimensions of a tensor according to a specified permutation tuple while maintaining the data's order.\n",
        "\n",
        "    Parameters:\n",
        "    tensors (tuple): tuple of Tensors with same shape except in concatenate dimension.\n",
        "\n",
        "    dim (int): concatenate dimension\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: concatenated tensor.\n",
        "    \"\"\"\n",
        "    return torch.cat((X, Y),2) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b9JuLVim-gZ"
      },
      "source": [
        "#### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEJXzoXRm-ga",
        "outputId": "1dab92c9-db76-4277-a451-7657a08ff678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[5, 0, 3, 3],\n",
            "         [7, 9, 3, 5],\n",
            "         [2, 4, 7, 6]],\n",
            "\n",
            "        [[8, 8, 1, 6],\n",
            "         [7, 7, 8, 1],\n",
            "         [5, 9, 8, 9]]])\n",
            "tensor([[[0, 0, 1],\n",
            "         [2, 0, 2],\n",
            "         [0, 1, 1]],\n",
            "\n",
            "        [[2, 0, 1],\n",
            "         [1, 1, 0],\n",
            "         [2, 0, 2]]])\n",
            "tensor([[[5, 0, 3, 3, 0, 0, 1],\n",
            "         [7, 9, 3, 5, 2, 0, 2],\n",
            "         [2, 4, 7, 6, 0, 1, 1]],\n",
            "\n",
            "        [[8, 8, 1, 6, 2, 0, 1],\n",
            "         [7, 7, 8, 1, 1, 1, 0],\n",
            "         [5, 9, 8, 9, 2, 0, 2]]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 10, size=(2,3,4))\n",
        "Y = np.random.randint(0, 3, size=(2,3,3))\n",
        "X = numpy2tensor(X)\n",
        "Y = numpy2tensor(Y)\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(PYTORCH_concatenate((X,Y),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nui-chnJm-ga"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_concatenate(tensors,dim) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [[[5,&nbsp;0,&nbsp;3,&nbsp;3,&nbsp;0,&nbsp;0,&nbsp;1], <br>\n",
        "            &nbsp; [7,&nbsp;9,&nbsp;3,&nbsp;5,&nbsp;2,&nbsp;0,&nbsp;2], <br>\n",
        "            &nbsp; [2,&nbsp;4,&nbsp;7,&nbsp;6,&nbsp;0,&nbsp;1,&nbsp;1]],<br>\n",
        "            <br>\n",
        "            &nbsp;[[8,&nbsp;8,&nbsp;1,&nbsp;6,&nbsp;2,&nbsp;0,&nbsp;1], <br>\n",
        "            &nbsp; [7,&nbsp;7,&nbsp;8,&nbsp;1,&nbsp;1,&nbsp;1,&nbsp;0], <br>\n",
        "            &nbsp; [5,&nbsp;9,&nbsp;8,&nbsp;9,&nbsp;2,&nbsp;0,&nbsp;2]]],<br>\n",
        "       \n",
        "        \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAnK2um8m-ga"
      },
      "source": [
        "#### 3.8 Stack\n",
        "\n",
        "In this task, you will implement the stack function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_stack`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD4g9dXoeZOJ"
      },
      "source": [
        "Tensor 1: tensor([1, 2, 3])\n",
        "Tensor 2: tensor([4, 5, 6])\n",
        "Stacked tensor: tensor([[1, 4],\n",
        "                        [2, 5],\n",
        "                        [3, 6]])\n",
        "Stacked tensor shape: torch.Size([3, 2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nOYp8Mr0m-ga"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_stack(tensors,dim):\n",
        "\n",
        "    \"\"\"\n",
        "    In contrast to Concatenation, which merges two tensors along an existing dimension,\n",
        "    Stack creates a new dimension to combine tensors.\n",
        "\n",
        "    Parameters:\n",
        "    tensors (tuple):  tuple of tensors to be stacked\n",
        "    dim (int): dimension to insert.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: stacked tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.stack((X, Y), 1) #TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Q152R9m-ga"
      },
      "source": [
        "#### Test Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hZe54j-m-ga",
        "outputId": "6f5f94be-1618-44a1-a15f-d6df2f2aeabb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[5, 0, 3],\n",
            "        [3, 7, 9]])\n",
            "tensor([[1, 2, 0],\n",
            "        [2, 0, 0]])\n",
            "tensor([[[5, 0, 3],\n",
            "         [1, 2, 0]],\n",
            "\n",
            "        [[3, 7, 9],\n",
            "         [2, 0, 0]]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 10, size=(2,3))\n",
        "Y = np.random.randint(0, 3, size=(2,3))\n",
        "X = numpy2tensor(X)\n",
        "Y = numpy2tensor(Y)\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(PYTORCH_stack((X,Y),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHG_DL7ym-ga"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_stack(tensors,dims) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "            [[[5,&nbsp;0,&nbsp;3,], <br>\n",
        "        &nbsp; [1,&nbsp;2,&nbsp;0,]], <br>\n",
        "        <br>\n",
        "        &nbsp;[[3,&nbsp;7,&nbsp;9,], <br>\n",
        "        &nbsp; [2,&nbsp;0,&nbsp;0,]]], <br>\n",
        "        <br>\n",
        "        \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fexVGEz_m-ga"
      },
      "source": [
        "#### 3.9 Padding\n",
        "\n",
        "In this task, you will implement the padding  function for torch tensors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Your Task:** Implement the function `PYTORCH_padding`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ1h-_NIg72n"
      },
      "source": [
        "\n",
        "By unpacking this tuple, we obtain pad_cols_before, pad_cols_after, pad_rows_before, pad_rows_after, pad_channels_before, and pad_channels_after.\n",
        "Padding the Tensor:\n",
        "The pad argument of torch.nn.functional.pad() takes a tuple of padding values for each dimension in the order (left, right, top, bottom, front, back), corresponding to columns, rows, and channels respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DAyLlSBom-ga"
      },
      "outputs": [],
      "source": [
        "def PYTORCH_padding(X,pad_widths):\n",
        "\n",
        "    \"\"\"\n",
        "    This involves adding extra elements (usually zeros) around the edges of a tensor.\n",
        "    We will encounter this during convolutional operations to control the spatial dimensions of the output.\n",
        "\n",
        "    Parameters:\n",
        "    X (torch.Tensor): input tensor which is to be padded.\n",
        "    pad_widths (tuple): even number of elements in tuple that represents padding widths in the order columns,rows and channels.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: stacked tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.nn.functional.pad(X, (2,2,1,1,0,0)) #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5AlNAnim-gb",
        "outputId": "793823ab-41b9-4b27-d235-016d1bc1c02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[6, 1],\n",
            "         [4, 4]],\n",
            "\n",
            "        [[8, 4],\n",
            "         [6, 3]]])\n",
            "tensor([[[0, 0, 0, 0, 0, 0],\n",
            "         [0, 0, 6, 1, 0, 0],\n",
            "         [0, 0, 4, 4, 0, 0],\n",
            "         [0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0],\n",
            "         [0, 0, 8, 4, 0, 0],\n",
            "         [0, 0, 6, 3, 0, 0],\n",
            "         [0, 0, 0, 0, 0, 0]]])\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.randint(1, 10, size=(2,2,2))\n",
        "\n",
        "X = numpy2tensor(X)\n",
        "\n",
        "print(X)\n",
        "print(PYTORCH_padding(X,(2,2,1,1,0,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hna8R6vdm-gb"
      },
      "source": [
        "**Expected Output**:\n",
        "<table style = \"align:40%\">\n",
        "    <tr>\n",
        "        <td style=\"text-align:left;\"><tt><b> PYTORCH_padding(X,pad_widths) </b></tt></td>\n",
        "        <td style=\"text-align:left;\"><tt>\n",
        "     [[[0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0],<br>\n",
        "&nbsp; [0,&nbsp;0,&nbsp;6,&nbsp;1,&nbsp;0,&nbsp;0], <br>\n",
        "&nbsp; [0,&nbsp;0,&nbsp;4,&nbsp;4,&nbsp;0,&nbsp;0], <br>\n",
        "&nbsp; [0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0]] <br>\n",
        "<br>\n",
        "&nbsp;[[0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0], <br>\n",
        "&nbsp; [0,&nbsp;0,&nbsp;8,&nbsp;4,&nbsp;0,&nbsp;0], <br>\n",
        "&nbsp; [0,&nbsp;0,&nbsp;6,&nbsp;3,&nbsp;0,&nbsp;0], <br>\n",
        "&nbsp; [0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;0]]]\n",
        "        </tt></td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 10, 'model_state_dict': OrderedDict([('model.0.weight', tensor([[ 3.5128e-02, -4.7693e-02, -3.3349e-02,  ..., -4.1407e-02,\n",
            "          2.2893e-02,  1.5576e-02],\n",
            "        [-4.9611e-02, -9.7291e-03,  3.2748e-02,  ..., -4.1319e-02,\n",
            "         -4.5200e-02, -3.4095e-02],\n",
            "        [-5.9416e-03,  5.0342e-02, -3.1828e-02,  ...,  3.7495e-02,\n",
            "          1.4026e-02, -2.8153e-02],\n",
            "        ...,\n",
            "        [-1.7419e-02,  4.8331e-02,  3.3734e-02,  ...,  4.5839e-02,\n",
            "          4.8207e-02,  7.6556e-06],\n",
            "        [-2.4018e-02, -2.1531e-02,  4.0233e-02,  ...,  4.8539e-02,\n",
            "         -5.0184e-02,  3.3493e-02],\n",
            "        [-2.7406e-02, -4.9787e-02, -1.4970e-02,  ...,  3.3914e-02,\n",
            "          1.7436e-02, -3.5445e-02]])), ('model.0.bias', tensor([-0.0089, -0.1422,  0.0790,  ..., -0.1243, -0.0279, -0.0361])), ('model.2.weight', tensor([1., 1., 1.,  ..., 1., 1., 1.])), ('model.2.bias', tensor([0., 0., 0.,  ..., 0., 0., 0.])), ('model.2.running_mean', tensor([0., 0., 0.,  ..., 0., 0., 0.])), ('model.2.running_var', tensor([1., 1., 1.,  ..., 1., 1., 1.])), ('model.2.num_batches_tracked', tensor(0)), ('model.4.weight', tensor([[ 0.0052,  0.0423, -0.0035,  ...,  0.0145,  0.0080, -0.0029],\n",
            "        [ 0.0349,  0.0221,  0.0214,  ..., -0.0022,  0.0027,  0.0484],\n",
            "        [-0.0474,  0.0233, -0.0359,  ..., -0.0461, -0.0281,  0.0005],\n",
            "        ...,\n",
            "        [ 0.0006, -0.0216,  0.0035,  ..., -0.0394, -0.0218,  0.0177],\n",
            "        [-0.0303, -0.0017, -0.0319,  ...,  0.0337, -0.0238,  0.0290],\n",
            "        [-0.0359, -0.0022, -0.0463,  ...,  0.0399,  0.0285, -0.0407]])), ('model.4.bias', tensor([ 2.1206e-03, -1.4237e-02, -1.5060e-02, -2.1277e-02,  2.1047e-02,\n",
            "         1.1609e-02,  1.8004e-02,  1.1189e-02, -3.1182e-04,  2.0341e-02,\n",
            "         6.9377e-04, -1.6474e-02, -5.9259e-03,  1.2791e-03,  3.2557e-03,\n",
            "        -1.7911e-02, -6.7623e-03, -1.2815e-02,  1.1882e-02,  1.4264e-02,\n",
            "        -2.0013e-02,  1.7556e-02, -1.2104e-02, -1.8010e-02,  4.0312e-04,\n",
            "         7.8972e-03, -5.4953e-03, -1.5599e-02, -1.3132e-02,  1.8008e-02,\n",
            "        -8.3440e-03,  1.8584e-02, -1.4470e-02, -2.1165e-02,  4.1735e-03,\n",
            "        -1.4881e-02, -2.1373e-02,  9.8463e-03, -1.5010e-02, -6.1045e-03,\n",
            "        -7.6543e-03,  1.7442e-02, -4.7319e-03,  1.4975e-02,  1.1905e-02,\n",
            "        -1.2871e-02,  2.0684e-02,  2.0789e-02, -1.7295e-02, -1.4211e-02,\n",
            "         1.8865e-02,  7.4407e-03,  2.4425e-03, -1.3088e-02, -2.0078e-03,\n",
            "         2.7000e-04,  1.9122e-02, -1.1166e-02,  1.2804e-02,  1.3761e-02,\n",
            "         1.6627e-02,  1.5225e-02,  2.5830e-04, -1.2499e-02, -8.3230e-03,\n",
            "        -1.4825e-02, -8.4896e-03, -6.0965e-03, -8.8038e-03, -1.0870e-02,\n",
            "         6.8639e-03, -1.7097e-03,  8.2436e-04, -7.5087e-03,  1.6614e-02,\n",
            "         1.1051e-02, -1.0882e-03,  1.3956e-02,  1.8867e-02,  1.3594e-02,\n",
            "        -9.7687e-03, -1.3932e-02, -2.0807e-02, -1.1987e-02,  2.4473e-03,\n",
            "        -1.3016e-02,  7.0146e-03,  8.3248e-03, -6.3247e-03,  9.7024e-03,\n",
            "        -1.8546e-02,  5.0312e-03, -5.1427e-03, -7.8705e-03,  1.2296e-03,\n",
            "         7.0747e-03,  1.8756e-02,  1.1119e-02,  8.8744e-03, -2.1977e-03,\n",
            "         2.1654e-02, -1.4122e-02, -3.9020e-03, -1.1697e-02, -6.7789e-03,\n",
            "         7.0266e-03,  3.4228e-03,  2.0929e-02, -1.7586e-02, -8.5203e-04,\n",
            "        -1.1120e-02,  4.3773e-03, -1.4195e-02,  9.1847e-03, -3.6765e-03,\n",
            "         9.1035e-03, -2.0823e-02, -1.1104e-02, -1.2863e-02, -6.9946e-03,\n",
            "        -1.5015e-02,  1.6940e-02, -1.6744e-02,  5.1904e-03, -1.8627e-02,\n",
            "        -8.5273e-03,  7.6784e-04, -1.1467e-03,  1.3661e-02, -9.5033e-03,\n",
            "         7.6214e-03,  4.5725e-03, -1.9568e-02, -2.4516e-03, -1.6597e-02,\n",
            "         1.3381e-02,  3.7042e-03, -1.5317e-02,  1.3941e-02, -9.5726e-03,\n",
            "        -1.7888e-02, -5.4808e-03, -9.1662e-03,  1.5712e-02, -1.3045e-02,\n",
            "        -9.9522e-03, -8.2224e-03,  7.1287e-03,  1.9449e-02, -1.5179e-02,\n",
            "         2.0398e-02, -1.7985e-03, -7.2749e-04, -1.0075e-02,  1.1014e-02,\n",
            "        -4.3556e-03, -8.5794e-03, -7.6389e-03, -3.2129e-04, -4.7278e-03,\n",
            "         1.8132e-03, -1.9240e-03, -1.3990e-02, -5.7014e-03, -2.0533e-02,\n",
            "        -2.3221e-03,  1.8720e-02, -1.2989e-02,  1.3247e-02, -4.0298e-03,\n",
            "         9.2089e-03,  1.3821e-02,  2.2904e-03,  2.1659e-02,  1.9728e-02,\n",
            "        -1.7200e-03, -1.5794e-02,  9.0158e-03,  1.0738e-02,  1.8061e-02,\n",
            "         4.3166e-03,  6.1458e-03, -1.6205e-02, -7.2946e-03, -8.6947e-03,\n",
            "         2.1449e-02,  7.7950e-03,  1.5997e-02,  1.7381e-02,  9.9709e-03,\n",
            "        -1.1943e-02,  1.6258e-02, -1.8171e-02,  1.7135e-02, -2.0614e-02,\n",
            "         2.3743e-04, -1.5335e-02, -1.4537e-02,  1.1515e-02,  1.0089e-02,\n",
            "         6.3191e-03, -1.9882e-02,  1.6160e-03,  6.1746e-04, -1.9445e-02,\n",
            "         1.8805e-04,  2.0096e-02,  1.2798e-02,  1.9018e-03, -6.2620e-03,\n",
            "         1.3814e-02, -2.9620e-03,  1.0291e-02,  2.0268e-02, -7.8641e-03,\n",
            "         1.1648e-02, -1.2189e-02,  2.5131e-03, -1.8272e-02, -6.6658e-03,\n",
            "         1.0906e-02,  1.0117e-02,  1.8417e-02, -1.8546e-02, -1.7292e-03,\n",
            "         2.2469e-03,  1.6783e-02,  1.3044e-02, -1.9734e-02, -1.5258e-02,\n",
            "         2.0408e-02,  3.9796e-03, -5.8006e-03, -2.0204e-02,  1.4090e-02,\n",
            "         4.3383e-03,  1.7623e-02,  2.1115e-02,  5.9676e-03,  1.4864e-02,\n",
            "        -8.1441e-03, -1.0459e-02, -1.9313e-02,  9.1432e-03,  3.6272e-03,\n",
            "         8.5262e-03, -1.8724e-02,  1.0696e-02, -1.3183e-03,  9.9401e-03,\n",
            "         4.3761e-03,  1.1216e-02,  4.2061e-03,  1.9404e-02,  9.7419e-03,\n",
            "        -1.6289e-02, -1.0111e-02, -1.5425e-02,  1.5687e-02,  1.7144e-02,\n",
            "         6.0560e-03,  3.8057e-03,  1.8402e-03,  1.4029e-02,  9.5404e-04,\n",
            "         7.4669e-03,  1.5781e-02,  1.1000e-02, -1.5359e-02, -2.8687e-03,\n",
            "         3.8953e-03,  4.9410e-03, -1.2150e-02, -3.0640e-03,  2.0778e-02,\n",
            "         4.6598e-03,  2.0652e-02,  4.7703e-03,  5.2117e-03,  1.3387e-02,\n",
            "        -1.4473e-03,  1.6065e-03, -9.9175e-03, -1.3575e-02,  7.9117e-03,\n",
            "        -2.0720e-02, -2.9968e-03, -4.5108e-03, -1.7696e-02, -1.4754e-02,\n",
            "        -2.0347e-02,  7.3563e-03,  1.7698e-02, -1.4498e-02, -6.9612e-03,\n",
            "        -5.3247e-03,  2.0136e-02,  1.0500e-02,  1.3050e-02,  1.6654e-03,\n",
            "        -6.6629e-03,  1.0502e-03,  2.0675e-02, -2.7965e-03, -9.9656e-03,\n",
            "         1.4872e-03, -2.1367e-02,  1.8206e-02,  1.9962e-02,  1.1506e-02,\n",
            "         4.4680e-03, -1.8190e-02,  9.0974e-03,  1.6422e-02,  1.1530e-02,\n",
            "         2.0556e-02,  7.0965e-03,  2.1210e-02,  1.7110e-02,  2.0896e-02,\n",
            "         2.0477e-02,  2.7671e-03,  2.0788e-02, -1.7853e-02, -1.2799e-02,\n",
            "        -1.6596e-02,  1.7517e-02,  2.5506e-03, -1.3366e-03,  2.0087e-02,\n",
            "         1.8023e-02, -8.3652e-03,  1.4517e-02,  1.1454e-02, -2.2032e-02,\n",
            "         9.0386e-03,  1.6364e-02, -1.9841e-02, -1.1683e-02, -6.0372e-03,\n",
            "         1.6677e-02,  1.9139e-03,  1.0062e-02,  1.8929e-03, -1.3358e-02,\n",
            "         3.6524e-03,  2.1176e-02, -1.6869e-02, -8.2407e-03, -1.1752e-02,\n",
            "         1.8181e-02, -5.6439e-03,  2.1461e-02,  1.4189e-02,  2.1884e-02,\n",
            "         2.0655e-02, -1.5828e-02, -1.1801e-02,  1.4862e-02, -1.4903e-02,\n",
            "         6.7424e-04, -8.9038e-03, -1.3870e-02, -1.0796e-02, -1.9316e-02,\n",
            "         1.4324e-02, -1.6081e-02,  5.5419e-03, -1.6100e-02,  6.8422e-03,\n",
            "        -1.9554e-02,  1.0080e-02, -2.0061e-02, -1.2396e-02, -1.7805e-02,\n",
            "         1.5074e-02,  2.1359e-02,  1.0593e-03, -1.4165e-02,  4.3907e-03,\n",
            "         8.0509e-03,  1.7937e-02,  2.2021e-02,  9.2315e-03, -1.7074e-02,\n",
            "         7.1684e-03,  1.5046e-02, -1.2639e-02,  9.7796e-03, -5.0830e-03,\n",
            "        -8.0752e-03, -3.0336e-03,  1.6341e-02,  1.8348e-02,  6.2899e-03,\n",
            "         6.0349e-03, -6.8876e-03,  7.7361e-03, -1.9395e-02,  1.1342e-02,\n",
            "         2.0042e-02, -7.5415e-03,  1.7397e-02,  9.5038e-03, -8.5320e-03,\n",
            "         2.0810e-02, -1.6396e-02, -1.4393e-02, -1.9574e-02,  2.3767e-03,\n",
            "        -1.9360e-03, -2.6118e-04, -1.2805e-02, -1.5616e-02,  1.9134e-02,\n",
            "         6.9507e-03, -9.2705e-03, -4.2339e-03, -7.0648e-03, -1.7833e-02,\n",
            "        -8.9638e-03, -1.5976e-02,  1.7053e-02, -6.1275e-03,  3.8780e-03,\n",
            "        -7.0825e-03,  9.8514e-03, -1.5258e-02, -1.2200e-02,  1.8101e-02,\n",
            "         1.7926e-02,  1.9731e-02,  4.6651e-03,  4.9633e-03,  4.7510e-03,\n",
            "         1.9410e-02, -1.3840e-02,  1.4397e-02,  2.0470e-02, -1.3149e-02,\n",
            "         3.1638e-04, -1.8724e-02,  3.5663e-03,  1.3247e-03,  4.9527e-03,\n",
            "         2.1685e-02, -1.9470e-03,  1.3587e-02, -1.3830e-02, -8.3350e-03,\n",
            "        -1.4379e-02, -1.6936e-02, -1.3480e-02, -8.7328e-03,  2.4104e-03,\n",
            "        -2.2654e-06, -1.9722e-02, -1.7737e-02,  7.7750e-03,  2.0080e-02,\n",
            "         1.3765e-02,  1.8041e-02, -1.1915e-03, -1.7478e-02,  3.4187e-03,\n",
            "         5.9197e-03,  9.8878e-04, -1.1560e-02, -1.2731e-02, -1.8314e-02,\n",
            "        -1.7795e-02, -1.4023e-02, -7.7214e-03, -1.0681e-03,  8.1548e-03,\n",
            "        -2.1290e-02, -1.8584e-02,  3.4456e-03, -8.9241e-03,  1.3784e-02,\n",
            "        -1.3785e-02, -1.6736e-02,  3.9460e-03, -5.3736e-03,  8.7040e-03,\n",
            "        -8.7899e-03, -7.3409e-03, -2.2069e-02, -2.1688e-03, -6.0288e-03,\n",
            "         1.8437e-02,  6.5597e-03,  2.0698e-02,  2.0799e-02, -6.3579e-03,\n",
            "        -7.4622e-04,  1.9638e-02,  6.5918e-03, -1.3572e-02, -1.9012e-02,\n",
            "        -2.4549e-04, -1.4814e-02,  2.5303e-03,  1.0558e-02, -4.7772e-03,\n",
            "        -5.1376e-03,  1.3348e-02,  4.4034e-03, -2.0888e-02,  9.1119e-03,\n",
            "         1.6856e-02, -7.8177e-03])), ('model.6.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1.])), ('model.6.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])), ('model.6.running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.])), ('model.6.running_var', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1.])), ('model.6.num_batches_tracked', tensor(0)), ('model.8.weight', tensor([[-0.0437,  0.0815,  0.0594,  ..., -0.0458,  0.0884, -0.0356],\n",
            "        [-0.0005, -0.0042, -0.0163,  ...,  0.0840,  0.0534, -0.0818],\n",
            "        [ 0.0846,  0.0155,  0.0431,  ...,  0.0592,  0.0021,  0.0741],\n",
            "        ...,\n",
            "        [ 0.0054, -0.0228,  0.0713,  ..., -0.0399,  0.0553,  0.0740],\n",
            "        [-0.0736,  0.0037,  0.0750,  ..., -0.0373,  0.0801,  0.0279],\n",
            "        [-0.0532,  0.0571,  0.0858,  ...,  0.0078,  0.0706, -0.0318]])), ('model.8.bias', tensor([-0.0130,  0.0234, -0.0023,  0.0077, -0.0409, -0.0062, -0.0176, -0.0231,\n",
            "         0.0059,  0.0027,  0.0356, -0.0016,  0.0102, -0.0275,  0.0125, -0.0183,\n",
            "        -0.0200, -0.0164, -0.0347, -0.0155,  0.0273, -0.0130,  0.0086,  0.0402,\n",
            "        -0.0278, -0.0411,  0.0008,  0.0110, -0.0415, -0.0077, -0.0201,  0.0365,\n",
            "         0.0037, -0.0035, -0.0405,  0.0186, -0.0125,  0.0314,  0.0401,  0.0297,\n",
            "        -0.0295,  0.0104,  0.0405, -0.0242,  0.0437,  0.0108, -0.0102,  0.0307,\n",
            "         0.0194,  0.0249, -0.0065,  0.0131, -0.0299,  0.0358, -0.0029, -0.0136,\n",
            "         0.0417,  0.0120, -0.0271, -0.0219,  0.0088,  0.0404,  0.0348,  0.0121,\n",
            "         0.0252, -0.0037, -0.0366, -0.0148,  0.0317, -0.0359, -0.0268, -0.0060,\n",
            "        -0.0082,  0.0131,  0.0364,  0.0404,  0.0338, -0.0177, -0.0164,  0.0380,\n",
            "        -0.0438,  0.0277, -0.0023, -0.0089, -0.0398,  0.0024,  0.0086,  0.0298,\n",
            "         0.0059, -0.0196, -0.0178,  0.0154,  0.0179, -0.0059,  0.0290, -0.0203,\n",
            "         0.0326, -0.0243, -0.0298,  0.0164,  0.0391, -0.0407,  0.0319, -0.0013,\n",
            "         0.0123, -0.0417,  0.0271, -0.0192,  0.0390,  0.0366, -0.0317,  0.0359,\n",
            "        -0.0272, -0.0185, -0.0068, -0.0160,  0.0100, -0.0287, -0.0083, -0.0021,\n",
            "        -0.0295, -0.0436, -0.0276, -0.0027,  0.0135, -0.0042, -0.0368, -0.0111,\n",
            "        -0.0344, -0.0175, -0.0146,  0.0236,  0.0280,  0.0238,  0.0386,  0.0274,\n",
            "         0.0165, -0.0109, -0.0200, -0.0140,  0.0239, -0.0117, -0.0288, -0.0032,\n",
            "         0.0139, -0.0417, -0.0316, -0.0249,  0.0292, -0.0081, -0.0028, -0.0141,\n",
            "        -0.0044, -0.0346, -0.0059, -0.0339, -0.0051, -0.0404, -0.0417, -0.0012,\n",
            "        -0.0004, -0.0182,  0.0099,  0.0411, -0.0441,  0.0119,  0.0086,  0.0290,\n",
            "         0.0290,  0.0421, -0.0377,  0.0072,  0.0406,  0.0293,  0.0284,  0.0029,\n",
            "         0.0050, -0.0191,  0.0256,  0.0213,  0.0386,  0.0316, -0.0355, -0.0230,\n",
            "        -0.0127,  0.0411,  0.0267,  0.0352,  0.0390, -0.0040,  0.0236, -0.0186,\n",
            "        -0.0313, -0.0011,  0.0091,  0.0119, -0.0227,  0.0013,  0.0131,  0.0003,\n",
            "         0.0086,  0.0399,  0.0080, -0.0233,  0.0400, -0.0110, -0.0127, -0.0162,\n",
            "        -0.0121, -0.0147,  0.0378,  0.0170, -0.0134,  0.0272, -0.0169, -0.0173,\n",
            "        -0.0199,  0.0383, -0.0292, -0.0129,  0.0338,  0.0304,  0.0292,  0.0253,\n",
            "        -0.0351, -0.0326, -0.0161, -0.0254,  0.0343,  0.0028, -0.0168,  0.0419,\n",
            "        -0.0106, -0.0212, -0.0385, -0.0031, -0.0127, -0.0041,  0.0329,  0.0240,\n",
            "         0.0206,  0.0211,  0.0062,  0.0042, -0.0251,  0.0334, -0.0434,  0.0109,\n",
            "        -0.0090, -0.0126, -0.0237, -0.0244,  0.0374, -0.0375, -0.0201, -0.0120])), ('model.10.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1.])), ('model.10.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), ('model.10.running_mean', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), ('model.10.running_var', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1.])), ('model.10.num_batches_tracked', tensor(0)), ('model.12.weight', tensor([[-0.1023, -0.0336, -0.0900,  ..., -0.0476,  0.0779,  0.0379],\n",
            "        [ 0.0942,  0.0864,  0.0477,  ...,  0.0902,  0.0297, -0.1233],\n",
            "        [ 0.0740,  0.0192,  0.0700,  ..., -0.1203, -0.0711, -0.0191],\n",
            "        ...,\n",
            "        [ 0.1282, -0.1182, -0.0350,  ..., -0.0492,  0.0011, -0.0017],\n",
            "        [ 0.0326,  0.0791, -0.0852,  ..., -0.1027, -0.1173, -0.0839],\n",
            "        [ 0.1327, -0.0149,  0.0292,  ...,  0.0969,  0.0054,  0.0029]])), ('model.12.bias', tensor([-0.0006, -0.0248, -0.0149,  0.0621, -0.0130, -0.0161, -0.0259, -0.0348,\n",
            "         0.0233, -0.0107, -0.0335,  0.0248,  0.0068, -0.0414, -0.0276,  0.0332,\n",
            "         0.0038,  0.0262, -0.0324, -0.0219,  0.0404,  0.0079,  0.0153,  0.0206,\n",
            "        -0.0282,  0.0052,  0.0310, -0.0031,  0.0417,  0.0231,  0.0571, -0.0330,\n",
            "         0.0082,  0.0612,  0.0448,  0.0278,  0.0531,  0.0548, -0.0444,  0.0252,\n",
            "         0.0302,  0.0096, -0.0105, -0.0162, -0.0051,  0.0508,  0.0505,  0.0293,\n",
            "         0.0222, -0.0149,  0.0525,  0.0310,  0.0287, -0.0255, -0.0509, -0.0589,\n",
            "         0.0073, -0.0031, -0.0357,  0.0605, -0.0089, -0.0184,  0.0480, -0.0056,\n",
            "        -0.0239, -0.0569,  0.0337,  0.0505,  0.0393,  0.0332,  0.0545]))]), 'loss': 0.001}\n"
          ]
        }
      ],
      "source": [
        "# Load the file\n",
        "pt_file = torch.load(\"./checkpoint.pt\")\n",
        "\n",
        "# Print the head of the file\n",
        "print(pt_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "cve",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
